{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "AsuD_CAPTUM_v0.1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianSchmidtke/CAPTUM/blob/main/AsuD_CAPTUM_v0.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBuZujwlr9cn"
      },
      "source": [
        "# pip installs. Wenn Schon installiert, dann einfach nicht ausführen\n",
        "!pip install networkx\n",
        "\n",
        "!pip install spacy\n",
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bionlp13cg_md-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n",
        "\n",
        "!pip install wordcloud\n",
        "\n",
        "!pip install -q flair\n",
        "!pip install -q numpy\n",
        "!pip install -q pandas\n",
        "!pip install -q gensim\n",
        "!pip install -q hdbscan\n",
        "!pip install -q bertopic\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow_hub\n",
        "!pip install -q tensorflow_text\n",
        "!pip install -q torch\n",
        "!pip install -q sentence_transformers"
      ],
      "id": "gBuZujwlr9cn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21aa3cc8"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import shutil"
      ],
      "id": "21aa3cc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1JNnvqwhn-d"
      },
      "source": [
        "# os.chdir(r'/content')\n",
        "\n",
        "# content_dir = r'/content/CAPTUM/'\n",
        "\n",
        "# # Remove if files already exist\n",
        "# while os.path.exists(content_dir): \n",
        "#     shutil.rmtree(content_dir)\n",
        "\n",
        "\n",
        "!git clone https://github.com/JulianSchmidtke/CAPTUM.git"
      ],
      "id": "y1JNnvqwhn-d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1twNnO_f3hp"
      },
      "source": [
        "# Duplikate unter den Papern entfernen anhand der Hash-Werte"
      ],
      "id": "R1twNnO_f3hp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18f27004"
      },
      "source": [
        "#Arbeitsverzeichnis auf das Ordnerverzeichnis ändern, in welchem die Daten liegen (txt-Format)\n",
        "file_dir = r'./CAPTUM/files'\n",
        "os.chdir(file_dir)\n",
        "\n",
        "#Speichern der Dateinamen in einem Array\n",
        "files = os.listdir()\n",
        "df_files = pd.DataFrame(files, columns=['Filepath'])"
      ],
      "id": "18f27004",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ugMSyU0f_4n"
      },
      "source": [
        "# create checksum for each file to identify duplicates\n",
        "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
        "import hashlib\n",
        "def get_checksum(filepath: str) -> str:\n",
        "    # Open,close, read file and calculate MD5 on its contents \n",
        "    with open(filepath, 'rb') as file_to_check:\n",
        "        # read contents of the file\n",
        "        data = file_to_check.read()    \n",
        "        # pipe contents of the file through\n",
        "        return hashlib.md5(data).hexdigest()\n",
        "df_files['Checksum'] = df_files['Filepath'].apply(get_checksum)"
      ],
      "id": "5ugMSyU0f_4n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62410ea9"
      },
      "source": [
        "fileNameTags = list(map(lambda fn: fn.replace('', '+').split('_')[0:-2],df_files['Filepath']))\n",
        "\n",
        "\n",
        "df_files['Name'] = [fNT[-1] for fNT in fileNameTags]\n",
        "df_files['Tags'] = [fNT[1:-1] for fNT in fileNameTags]\n",
        "df_files['Year'] = [fY[-8:-4] for fY in df_files['Name']]\n",
        "df_files['Authors'] = [fN.split(' ')[0] for fN in df_files['Name']]"
      ],
      "id": "62410ea9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6NJ23USltk9"
      },
      "source": [
        "#Den Inhalt der Paper einlesen und in einen Pandas-\n",
        "df_files['Content'] = df_files['Filepath'].apply(lambda f: open(f, 'r', encoding = \"ISO-8859-1\").read())\n",
        "df_files"
      ],
      "id": "-6NJ23USltk9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfbR4_P9s0f_"
      },
      "source": [
        "checksums = df_files['Checksum'].unique()\n",
        "for checksum in checksums:\n",
        "    tags = df_files.loc[df_files['Checksum'] == checksum, 'Tags']\n",
        "    list_of_pair_of_tags = tags.values.ravel()\n",
        "    list_of_tags = [tag for tags in list_of_pair_of_tags for tag in tags]\n",
        "    unique_tags = pd.unique(list_of_tags)\n",
        "    df_files.loc[df_files['Checksum'] == checksum, 'Tags'] = ', '.join(map(str, unique_tags))"
      ],
      "id": "NfbR4_P9s0f_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riAajBNjiMCf"
      },
      "source": [
        "# create dataframe for further analytics w/o the duplicate articles\n",
        "df = df_files.drop_duplicates(subset=['Checksum'], keep='first')\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df = df.loc[:,['Authors','Year','Tags', 'Content']]"
      ],
      "id": "riAajBNjiMCf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYlSFMEfgAwt"
      },
      "source": [
        "# Korpus erstellen"
      ],
      "id": "XYlSFMEfgAwt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFVLJt6FMzRr"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "id": "LFVLJt6FMzRr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nRxEmyLRhDU"
      },
      "source": [
        "Es werden Füllwörter entfernt."
      ],
      "id": "-nRxEmyLRhDU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "983b795c"
      },
      "source": [
        "stop_words=set(stopwords.words(\"english\"))\n",
        "def stoppingFunc(text, stop_words=stop_words):\n",
        "    filtered_text=[]\n",
        "    for word in text:\n",
        "        # Wort unter 3 Zeichen -> weg\n",
        "        if len(word) < 3:\n",
        "          next\n",
        "        # Mehr als eine Ziffer -> weg\n",
        "        elif sum(c.isdigit() for c in word)>1:\n",
        "          next\n",
        "        # nicht in Stopword Liste -> behalten\n",
        "        elif word not in stop_words:\n",
        "          filtered_text.append(word)\n",
        "        \n",
        "    return filtered_text"
      ],
      "id": "983b795c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wu3O5CFRfVx"
      },
      "source": [
        "Wörter auf ihren Wortstamm reduzieren (cats -> cat, transportation -> transport)"
      ],
      "id": "1Wu3O5CFRfVx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa7a4b64"
      },
      "source": [
        "def stemmingFunc(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in text]"
      ],
      "id": "fa7a4b64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDr_g2xlRdGG"
      },
      "source": [
        "Komplexere Version des Stemming. Wörter werden im Zusammenhang gesehen und reduziert. Verben werden auf den Infiitiv gesetzt usw."
      ],
      "id": "LDr_g2xlRdGG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efd6f40d"
      },
      "source": [
        "def lemmatizingFunc(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text]"
      ],
      "id": "efd6f40d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjUQ6ELhBLWv"
      },
      "source": [
        "def removeWords(text):\n",
        "  filtered_text=[]\n",
        "  for word in text:\n",
        "    if word.lower() in ('csu', 'urticaria', 'doi', 'http', 'the', 'al', 'fig', \n",
        "                        'point', 'data', 'patient', 'result', 'study', 'disease', \n",
        "                        'result', 'group', 'table', 'figure', 'patient', 'antihistamine',\n",
        "                        'cell', 'symptom'):\n",
        "      next\n",
        "    else:\n",
        "      filtered_text.append(word)\n",
        "\n",
        "  counter = Counter(filtered_text)\n",
        "  filtered_text = [key for key, val in counter.items() if val > 1]\n",
        "\n",
        "  return filtered_text"
      ],
      "id": "hjUQ6ELhBLWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXAdrQgT4sV"
      },
      "source": [
        "def remove_doi(text):\n",
        "  doi_regex = r\"10.\\d{4,9}\\/[-._;()\\/:A-Z0-9]+$\"\n",
        "  new_text = re.sub(doi_regex, '', text, flags=re.IGNORECASE)\n",
        "  return new_text"
      ],
      "id": "8jXAdrQgT4sV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTogJ6y6EOX4"
      },
      "source": [
        "removeWords(['csu', 'TEST', 'DOI', 'doi', 'al', 'DOI'])"
      ],
      "id": "QTogJ6y6EOX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKqD-XOFY8Xd"
      },
      "source": [
        "def normalize_text(tokenizer, text):\n",
        "  # lowercase text\n",
        "  text = str(text).lower()\n",
        "  # remove non-UTF\n",
        "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "  # remove doi\n",
        "  text = remove_doi(text)\n",
        "\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "  tokenized_text = removeWords(tokenized_text)\n",
        "  tokenized_text = stoppingFunc(tokenized_text)\n",
        "  #tokenized_text = stemmingFunc(tokenized_text)\n",
        "  tokenized_text = lemmatizingFunc(tokenized_text)\n",
        "\n",
        "  text = \" \".join(tokenized_text)\n",
        "  return text"
      ],
      "id": "xKqD-XOFY8Xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjs1JRBiRY-I"
      },
      "source": [
        "Ich glaube es sollte entweder eine Stemming oder eine Lemmatizing Funktion angewendet werden.\n",
        "\n",
        "Ich glaube wir sollten erst Lemmatizen und dann Stopwords entfernen."
      ],
      "id": "yjs1JRBiRY-I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ-PJE_p_K7g"
      },
      "source": [
        "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "df['Corpus'] = df['Content'].apply(lambda c: normalize_text(regex_tokenizer, c))\n",
        "\n",
        "regex_tokenizer2 = nltk.RegexpTokenizer('\\w+[\\.,]{0,1}')\n",
        "df['Corpus_w_Punctuation'] = df['Content'].apply(lambda c: normalize_text(regex_tokenizer2, c))\n",
        "\n",
        "# df['New_Corpus'] = df['Content'].apply(lambda c: simple_tokenizer(c))"
      ],
      "id": "EQ-PJE_p_K7g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxju8ng-_Gi5"
      },
      "source": [
        "# corpus_list = df['Content'].to_list()\n",
        "# remove_word_in_every_corpus('', corpus_list)"
      ],
      "id": "mxju8ng-_Gi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhxYmjBJ2iYG"
      },
      "source": [
        "df"
      ],
      "id": "HhxYmjBJ2iYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPVoM5XU5WGq"
      },
      "source": [
        "# Wordcloud\n"
      ],
      "id": "BPVoM5XU5WGq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLfPfxb45leF"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "id": "sLfPfxb45leF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV_k2aSr5YhZ"
      },
      "source": [
        "words = ' '.join(list(df['Corpus']))\n",
        "wordcloud = WordCloud(width=1000, height=1000, max_words=1000).generate(words)\n",
        "wordcloud.to_file(\"../../first_review.png\")\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "id": "NV_k2aSr5YhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCuEEMfP4-Y4"
      },
      "source": [
        "# Distinct Tags"
      ],
      "id": "hCuEEMfP4-Y4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StvfVkx226jA"
      },
      "source": [
        "type(list(df['Tags']))\n",
        "tags = set(', '.join(list(df['Tags'])).split(', '))\n",
        "print(*tags, sep=\"\\n\")"
      ],
      "id": "StvfVkx226jA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb35178"
      },
      "source": [
        "# Algorithmen anwenden"
      ],
      "id": "8bb35178"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4RAqPjcZSY4"
      },
      "source": [
        "### ScispaCy"
      ],
      "id": "T4RAqPjcZSY4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JlnzphOZVZ7"
      },
      "source": [
        ""
      ],
      "id": "6JlnzphOZVZ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTchhU9LZkIT"
      },
      "source": [
        "import spacy\n",
        "import scispacy\n",
        "\n",
        "from spacy import displacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.linking import EntityLinker\n",
        "from collections import OrderedDict, Counter\n",
        "from pprint import pprint\n"
      ],
      "id": "PTchhU9LZkIT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2CihlcPZrA1"
      },
      "source": [
        "\n",
        "def display_entities(model, document):\n",
        "  doc = nlp(document)\n",
        "  displacy_image = displacy.render(doc, jupyter=True, style='ent')\n",
        "  entity_and_label = set([(X.text, X.label_) for X in doc.ents])\n",
        "  return displacy_image, entity_and_label"
      ],
      "id": "N2CihlcPZrA1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QkOlsuvZzc4"
      },
      "source": [
        "#As an example print the first Corpus from the df with the recogniced entities.\n",
        "nlp = spacy.load('en_ner_bionlp13cg_md')\n",
        "entities = display_entities(nlp, df['Corpus_w_Punctuation'][0])"
      ],
      "id": "-QkOlsuvZzc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EKyYmRKaDLS"
      },
      "source": [
        "entities_dataframe = pd.DataFrame(entities[1],columns=['Entity','Label'])\n",
        "entities_dataframe"
      ],
      "id": "6EKyYmRKaDLS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hotGj2nZ1aK"
      },
      "source": [
        "def entity_linker(linker_name,document):\n",
        "  \"\"\"A function tha accepts the name of a scispacy knowledgebase and \n",
        "  documents and returns the entity link details\"\"\"\n",
        "  \n",
        "  doc = nlp(document)\n",
        "  try: \n",
        "    entity = doc.ents[0]\n",
        "  except IndexError:\n",
        "    entity = 'NaN'\n",
        "  entity_details = []\n",
        "  entity_details.append(entity)\n",
        "  try:\n",
        "    for linker_ent in entity._.kb_ents:\n",
        "      Concept_Id, Score = linker_ent\n",
        "      entity_details.appen('Entity_matching_Score :{}'.format(Score))\n",
        "      entity_details.append(linker.kb.cui_to_entity[linker_ent[linker_ent[0]]])\n",
        "  except AttributeError:\n",
        "    pass\n",
        "  return entity_details"
      ],
      "id": "5hotGj2nZ1aK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD8H-TZJeJv2"
      },
      "source": [
        "nlp = spacy.load('en_ner_bionlp13cg_md')\n",
        "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"}) #parameters are tunable, so it can be set to return more than 2 entity matches.\n",
        "doc = df['Corpus_w_Punctuation'][0]\n",
        "doc = nlp(doc)\n",
        "\n",
        "entity = doc.ents[0]\n",
        "\n",
        "linker = nlp.get_pipe(\"scispacy_linker\")\n",
        "for ent in entity._.kb_ents:\n",
        "\tprint(linker.kb.cui_to_entity[ent[0]])\n"
      ],
      "id": "FD8H-TZJeJv2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUvil67imP_C"
      },
      "source": [
        "### Bag of Words\n",
        "Quelle: [towardsdatascience](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
      ],
      "id": "NUvil67imP_C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXEW8CQQmqZA"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "np.random.seed(2021)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "id": "XXEW8CQQmqZA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhoqU6U_mue-"
      },
      "source": [
        "processed_docs = df['Corpus'].apply(lambda c: c.split(' '))"
      ],
      "id": "DhoqU6U_mue-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjoEt0rmwnV"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "id": "gWjoEt0rmwnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJf8rx6MoqDu"
      },
      "source": [
        "### **LDA** (LatentDirichletAllocation) \n",
        "Quelle: [Medium-Artikel zu LDA](https://towardsdatascience.com/nlp-with-lda-latent-dirichlet-allocation-and-text-clustering-to-improve-classification-97688c23d98)"
      ],
      "id": "vJf8rx6MoqDu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0i46zY1sXJ2"
      },
      "source": [
        "#Turn tags into a set for faster checking of whether a tag exists or not\n",
        "type(list(df['Tags']))\n",
        "unique_tags = set(', '.join(list(df['Tags'])).split(', '))\n",
        "print(*unique_tags, sep=\"\\n\")\n"
      ],
      "id": "j0i46zY1sXJ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMTf0IC6qfQi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "\n",
        "def find_topics(text):\n",
        "  count_vectorizer = CountVectorizer()\n",
        "  count_data = count_vectorizer.fit_transform(list(text))\n",
        "\n",
        "\n",
        "  # One topic that has an avg of two words because most questions had 1/2 tags\n",
        "  number_topics = 5\n",
        "  number_words = 20\n",
        "\n",
        "  # Create and fit the LDA model\n",
        "  lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "  lda.fit(count_data)\n",
        "  words = count_vectorizer.get_feature_names()\n",
        "\n",
        "  #Get topics from model. They are represented as a list e.g. ['military','army']\n",
        "  topics = [[words[i] for i in topic.argsort()[:-number_words - 1:-1]] for (topic_idx, topic) in enumerate(lda.components_)]\n",
        "  topic_groups = topics\n",
        "\n",
        "  topics = np.array(topics).ravel()\n",
        "  distinct_topics = list(set(topics))\n",
        "\n",
        "  #Only use topics for which a tag already exists\n",
        "  existing_topics = set.intersection(set(topics),unique_tags)\n",
        "\n",
        "  \n",
        "\n",
        "  return (words, distinct_topics, topic_groups, existing_topics)"
      ],
      "id": "iMTf0IC6qfQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVjwSYTVvfKI"
      },
      "source": [
        "words, distinct_topics, topic_groups, existing_topics = find_topics(df['Corpus_w_Punctuation'])"
      ],
      "id": "wVjwSYTVvfKI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hlQ-SwUzDG1"
      },
      "source": [
        "sorted(distinct_topics)\n"
      ],
      "id": "0hlQ-SwUzDG1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjf-SfSvy_hP"
      },
      "source": [
        "### **BERTopic** \n",
        "Topic modeling mit BERTopic -> siehe: https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6"
      ],
      "id": "rjf-SfSvy_hP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KkNASXt6JZ_"
      },
      "source": [
        "Copy and Paste aus: https://colab.research.google.com/drive/1iqs9Y5_zLI6R6mAwlnapcxcUbKjpv2CC?usp=sharing#scrollTo=MgMaef_uZT0T\n",
        "bzw.: https://www.youtube.com/watch?v=TLPmlVeEf1k"
      ],
      "id": "0KkNASXt6JZ_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdOCH0kszsfi"
      },
      "source": [
        "from bertopic import BERTopic"
      ],
      "id": "xdOCH0kszsfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj47Zp5HHis1"
      },
      "source": [
        "#Ursprüngliche Idee einen BiomedNLP-PubMedBERT zu nutzen um das Embedding zu erstellen.\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "\n",
        "#@title Transformer für das Embedding in BERtopic\n",
        "Transformer = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" #@param {type:\"string\"}\n",
        "LoadedTransformer = TransformerDocumentEmbeddings(Transformer)"
      ],
      "id": "fj47Zp5HHis1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRBqbFp4M-E3"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "\n",
        "glove_embedding = WordEmbeddings('crawl')\n",
        "document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])"
      ],
      "id": "YRBqbFp4M-E3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTFNj8VXCAQ"
      },
      "source": [
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True) # We need the probabilities to visualize\n",
        "topics = topic_model.fit_transform(list(df['Corpus_w_Punctuation']))"
      ],
      "id": "ELTFNj8VXCAQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFiD4jIi6DZn"
      },
      "source": [
        "# Get the most frequent topics\n",
        "topic_freq = topic_model.get_topic_freq()\n",
        "outliers = topic_freq['Count'][topic_freq['Topic']==-1].iloc[0]\n",
        "print(f\"{outliers} documents have not been classified\")\n",
        "print(f\"The other {topic_freq['Count'].sum() - outliers} documents are {topic_freq['Topic'].shape[0]-1} topics\")"
      ],
      "id": "bFiD4jIi6DZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVP7-geA6HXx"
      },
      "source": [
        "topic_freq"
      ],
      "id": "vVP7-geA6HXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgiYB3av6yeg"
      },
      "source": [
        "print(f\"There are {topic_freq['Count'].iloc[1]} documents that are talking about topic ID {topic_freq['Topic'].iloc[1]}\")"
      ],
      "id": "SgiYB3av6yeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EDrpSkB64rL"
      },
      "source": [
        "topic_model.get_topic(topic_freq['Topic'].iloc[0])"
      ],
      "id": "9EDrpSkB64rL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHDEzS4V7BBu"
      },
      "source": [
        "topic_model.visualize_topics()"
      ],
      "id": "HHDEzS4V7BBu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoXi6ici5_qs"
      },
      "source": [
        "### Top2Vec und Doc2Vec\n",
        "\n",
        "Quelle: https://github.com/bhattbhavesh91/Top2Vec-Demo/blob/main/Top2Vec-Notebook.ipynb"
      ],
      "id": "HoXi6ici5_qs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNR9_lgr6Rnw"
      },
      "source": [
        "!pip install top2vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from top2vec import Top2Vec"
      ],
      "id": "dNR9_lgr6Rnw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Rivd9R6bDL"
      },
      "source": [
        "docs = list(df[\"Corpus\"].values)\n",
        "docs[:5] #print top 5 papers"
      ],
      "id": "M0Rivd9R6bDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbrUw3TM7lRC"
      },
      "source": [
        "model = Top2Vec(docs, embedding_model='doc2vec')"
      ],
      "id": "XbrUw3TM7lRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3QTpYcV7s4U"
      },
      "source": [
        "topic_count = model.get_num_topics()"
      ],
      "id": "W3QTpYcV7s4U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51SEhzi-7xae"
      },
      "source": [
        "topic_words, word_scores, topic_nums = model.get_topics(topic_count)"
      ],
      "id": "51SEhzi-7xae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QwLqbcl76A_"
      },
      "source": [
        "for topic in topic_nums:\n",
        "    model.generate_topic_wordcloud(topic)"
      ],
      "id": "-QwLqbcl76A_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciiHqRed1eoi"
      },
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]"
      ],
      "id": "ciiHqRed1eoi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1zM4Fq0AsLG"
      },
      "source": [
        "#Erstellen einer Liste aller Wörter die im Corpus vorkommen\n",
        "corpus_collection_in_words = [corpus.split(' ') for corpus in df['Corpus']]\n",
        "flat_corpus_words = set(flatten(corpus_collection_in_words))\n",
        "flat_corpus_words\n",
        "\n",
        "keywords = [tag.split(' ') for tag in tags] #basierend auf den distict Tags der Corpora, tags entspricht dem zu beginn definierten Set.\n",
        "flat_keywords = set(flatten(keywords))\n",
        "\n",
        "searchable_keywords = set([word.lower() for word in flat_keywords if word.lower() in flat_corpus_words]) #Behalten der Keywords / Tags die selbst im Corpus auftreten."
      ],
      "id": "Z1zM4Fq0AsLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsKkbTuJxCIW"
      },
      "source": [
        "searchable_keywords"
      ],
      "id": "zsKkbTuJxCIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRSmUBR02VV"
      },
      "source": [
        "#@title Anzahl der ähnlichen Wörter pro gesuchtem Keyword\n",
        "number_of_similar_words =  50 #@param {type:\"integer\"}\n",
        "\n",
        "similarity_dict = []\n",
        "not_learned_by_model = []\n",
        "\n",
        "for word in searchable_keywords:\n",
        "  try: \n",
        "    words, word_scores = model.similar_words(keywords=[word], keywords_neg=[], num_words=number_of_similar_words)\n",
        "    similarity_dict.append({word:dict(zip(words,word_scores))})\n",
        "    next\n",
        "  except ValueError:\n",
        "    not_learned_by_model.append(word)\n",
        "print('The following words have not been learned by the model an thus are not part of the similarity search:')\n",
        "print(not_learned_by_model)\n",
        "    "
      ],
      "id": "eNRSmUBR02VV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7J_gb4eHi_7"
      },
      "source": [
        "similarity_dict"
      ],
      "id": "i7J_gb4eHi_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7411d8"
      },
      "source": [
        "### TFIDF Vertorizer"
      ],
      "id": "1c7411d8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2453744a"
      },
      "source": [
        "#@title Dateiinhalt, Korpus oder Korpus mit Satzzeichen\n",
        "src = 'Corpus' #@param {type:\"string\"}\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1)).fit(df[src].values)"
      ],
      "id": "2453744a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "362e41ab"
      },
      "source": [
        "len(tfidf_vectorizer.vocabulary_)"
      ],
      "id": "362e41ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95f044e2"
      },
      "source": [
        "tfidf_vectors = tfidf_vectorizer.fit_transform(df['Corpus'].values)"
      ],
      "id": "95f044e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhCN19SEVLDp"
      },
      "source": [
        "# get the first vector out (for the first document) \n",
        "first_vector_tfidfvectorizer=tfidf_vectors[0]\n",
        "df_tfidf_sample = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
        "df_tfidf_sample.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "id": "ZhCN19SEVLDp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6712dce8"
      },
      "source": [
        "### K-Means Clustering"
      ],
      "id": "6712dce8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67d59ddc"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "id": "67d59ddc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d1b144"
      },
      "source": [
        "K-Means für mehrere k's berechnen und anschließend plotten"
      ],
      "id": "95d1b144"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8567ca"
      },
      "source": [
        "Sum_of_squared_distances = []\n",
        "silhouettes = []\n",
        "max_range = 10\n",
        "K = range(2,max_range + 1)\n",
        "for k in K: \n",
        "    print(str(k) + \"/\" + str(max_range))\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km_labels = km.fit(tfidf_vectors)\n",
        "    Sum_of_squared_distances.append(km_labels.inertia_)\n",
        "    km_labels_2 = km.fit_predict(tfidf_vectors)\n",
        "    silhouette_avg = silhouette_score(tfidf_vectors, km_labels_2)\n",
        "    silhouettes.append(silhouette_avg)\n"
      ],
      "id": "3a8567ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49a1714b"
      },
      "source": [
        "plt.plot(K,Sum_of_squared_distances, 'b^-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Summe der quadrierten Abweichungen')\n",
        "plt.title('Elbow-Methode (Korpus mit Zeichensetzung)')\n",
        "plt.show()"
      ],
      "id": "49a1714b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5HHl9QB1Ef6"
      },
      "source": [
        "plt.plot(K,silhouettes, 'b^-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Silouette-Score')\n",
        "plt.title('Silouette-Score je Anzahl der Cluster')\n",
        "plt.show()"
      ],
      "id": "-5HHl9QB1Ef6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdOeFJslhT2V"
      },
      "source": [
        "Wortsammlungen erstellen durch angabe der Idealen Clusterzahl im Formularfeld"
      ],
      "id": "xdOeFJslhT2V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A6o-gMJYAKI"
      },
      "source": [
        "#@title Anzahl der Cluster gem. Elbow-Curve\n",
        "k = 8#@param {type:\"integer\"}\n",
        "\n",
        "km = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1,\n",
        "                verbose=1)"
      ],
      "id": "3A6o-gMJYAKI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFrzxFPxYiIn"
      },
      "source": [
        "#https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html\n",
        "km = km.fit(tfidf_vectors)\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "clustered_words = []\n",
        "for i in range(k):\n",
        "  words_per_cluster = []\n",
        "  for ind in order_centroids[i, :5]:\n",
        "    words_per_cluster.append(terms[ind])\n",
        "  clustered_words.append(words_per_cluster)"
      ],
      "id": "mFrzxFPxYiIn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poE8cdUgJnpI"
      },
      "source": [
        "#clustered_words Liste transponieren\n",
        "clustered_words = list(map(list, zip(*clustered_words)))\n",
        "\n",
        "#Dataframe aus clustered_words erstellen\n",
        "cluster_names = ['Cluster %s' %i for i in range(1,k+1)]\n",
        "df_clustered_words = pd.DataFrame(data=clustered_words, columns=cluster_names)\n",
        "df_clustered_words"
      ],
      "id": "poE8cdUgJnpI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQJfMViFsNdg"
      },
      "source": [
        "#### Visualisierung"
      ],
      "id": "PQJfMViFsNdg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alRnyKz0sNCj"
      },
      "source": [
        "import networkx as nx\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "words = []\n",
        "color_map = []\n",
        "\n",
        "for cluster in range(k):\n",
        "  for row in clustered_words:\n",
        "    if not G.has_node(row[cluster]):\n",
        "      G.add_node(row[cluster], color=cluster)\n",
        "      if cluster == 0:\n",
        "          color_map.append('blue')\n",
        "      elif cluster == 1: \n",
        "          color_map.append('green')\n",
        "      elif cluster == 2: \n",
        "          color_map.append('red')\n",
        "      elif cluster == 3: \n",
        "          color_map.append('yellow')\n",
        "      elif cluster == 4: \n",
        "          color_map.append('grey')\n",
        "      elif cluster == 5: \n",
        "          color_map.append('orange')\n",
        "      elif cluster == 6: \n",
        "          color_map.append('black')\n",
        "      elif cluster == 7: \n",
        "          color_map.append('brown')\n",
        "\n",
        "    words.append(row[cluster])\n",
        "\n",
        "corpus_list = df['Corpus'].to_list()\n",
        "\n",
        "for word1 in words:\n",
        "  for word2 in words:\n",
        "    for corpus in corpus_list:\n",
        "      if (word1 in corpus) and (word2 in corpus):\n",
        "        if G.has_edge(word1, word2):\n",
        "          G[word1][word2]['weight'] += 0.5\n",
        "        elif G.has_edge(word2, word1):\n",
        "          G[word2][word1]['weight'] += 0.5\n",
        "        else:\n",
        "          G.add_edge(word1, word2, weight=0.5)\n",
        "\n",
        "pos = nx.spring_layout(G, scale=2)\n",
        "nx.draw(G, pos=pos, with_labels=True, node_color=color_map)\n",
        "nx.write_gexf(G, 'C:/temp/gephy.gexf')"
      ],
      "id": "alRnyKz0sNCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mNgQBGbYqJs"
      },
      "source": [
        "### **HDBSCAN**"
      ],
      "id": "7mNgQBGbYqJs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVPZHY6XfB0X"
      },
      "source": [
        "#Es wurden die folgenden Transformer getestet:\n",
        "# 1. allenai/scibert_scivocab_uncased\n",
        "# 2. allenai/specter\n",
        "# 3. microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
        "\n",
        "#@title Transformer für Embedding\n",
        "transformer_name = \"allenai/specter\" #@param {type:\"string\"}\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(transformer_name)\n",
        "embeddings = model.encode(df['Corpus_w_Punctuation'], show_progress_bar=True)"
      ],
      "id": "VVPZHY6XfB0X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2xVsp0mcTzr"
      },
      "source": [
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=12, \n",
        "                            n_components=2,\n",
        "                            metric='cosine').fit_transform(embeddings)"
      ],
      "id": "i2xVsp0mcTzr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nhu1hksZRRu"
      },
      "source": [
        "import hdbscan\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,\n",
        "                          metric='euclidean',                      \n",
        "                          cluster_selection_method='eom')\n",
        "cluster = clusterer.fit(umap_embeddings)"
      ],
      "id": "2Nhu1hksZRRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkgWRswUgfIl"
      },
      "source": [
        "# Prepare data\n",
        "umap_data = umap.UMAP(n_neighbors=12, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
        "result['labels'] = cluster.labels_"
      ],
      "id": "qkgWRswUgfIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDjACwdYaU6f"
      },
      "source": [
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BEBEBE', s=5)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=20, cmap=plt.cm.get_cmap('hsv_r', len(set(clustered.labels))))\n",
        "plt.colorbar(ticks=range(len(set(clustered.labels))+1), label='Cluster')\n",
        "plt.axis('off')\n",
        "plt.clim(-0.5, (len(set(clustered.labels))+.5))"
      ],
      "id": "CDjACwdYaU6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgzBPYjQsFdE"
      },
      "source": [
        "# KMeans in simpel am Stück"
      ],
      "id": "QgzBPYjQsFdE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSuP8UeM7taD"
      },
      "source": [
        "stopWords = stopwords.words('english')\n",
        "\n",
        "additional_stopwords = ['csu','chronic', 'spontaneous', 'urticaria', 'doi', 'http', 'the', 'al', 'fig', \n",
        "                        'point', 'data', 'patient', 'result', 'study', 'disease', \n",
        "                        'result', 'group', 'table', 'figure', 'patient', 'antihistamine',\n",
        "                        'cell', 'symptom', 'crossref', 'pubm', 'activ', 'allergi', 'medi4212']\n",
        "\n",
        "charfilter = re.compile('[a-zA-Z]{4,}')\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "  text = remove_doi(text)\n",
        "  text = text.lower()\n",
        "  words = word_tokenize(text)\n",
        "  words = [word for word in words if word not in stopWords]\n",
        "  words = [word for word in words if word not in additional_stopwords]\n",
        "  tokens = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
        "  ntokens = list(filter(lambda token : charfilter.search(token),tokens))\n",
        "  ntokens = [word for word in ntokens if word not in additional_stopwords]\n",
        "  return ntokens\n",
        "\n",
        "pca = list(df['Content'])\n",
        "\n",
        "vec = TfidfVectorizer(tokenizer = simple_tokenizer, max_features = 1000, norm = 'l2', lowercase=True, ngram_range=(1,1))\n",
        "pca_vec = vec.fit_transform(pca)"
      ],
      "id": "fSuP8UeM7taD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DioAZ7dVN82M"
      },
      "source": [
        "# Tokenize Cluster again to compare\n",
        "pca_tokenized = list(df['Content'].apply(lambda e: simple_tokenizer(e)))"
      ],
      "id": "DioAZ7dVN82M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bou8EYPX9JxD"
      },
      "source": [
        "# KMeans berechnen für 20 Cluster\n",
        "Sum_of_squared_distances_simple = []\n",
        "silhouettes_simple = []\n",
        "max_range = 20\n",
        "K = range(2,max_range + 1)\n",
        "for k in K: \n",
        "    print(str(k) + \"/\" + str(max_range))\n",
        "    km = KMeans(n_clusters=k, max_iter=1000)\n",
        "    km_labels = km.fit(pca_vec)\n",
        "    Sum_of_squared_distances_simple.append(km_labels.inertia_)\n",
        "    km_labels_2 = km.fit_predict(pca_vec)\n",
        "    silhouette_avg = silhouette_score(pca_vec, km_labels_2)\n",
        "    silhouettes_simple.append(silhouette_avg)\n",
        "\n",
        "# Elbow für simples dataset\n",
        "plt.plot(K,Sum_of_squared_distances_simple, 'b^-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Summe der quadrierten Abweichungen')\n",
        "plt.title('Elbow-Methode')\n",
        "plt.show()\n",
        "\n",
        "# Silhouette für simples Dataset\n",
        "plt.plot(K,silhouettes_simple, 'b^-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Silouette-Score')\n",
        "plt.title('Silouette-Score je Anzahl der Cluster')\n",
        "plt.show()"
      ],
      "id": "Bou8EYPX9JxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyMTI42NCWxW"
      },
      "source": [
        "# KMeans Cluster\n",
        "k = 10\n",
        "k_means = KMeans(k, max_iter=100)\n",
        "clusters = k_means.fit_predict(pca_vec)\n",
        "\n",
        "order_centroids = k_means.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vec.get_feature_names()\n",
        "\n",
        "clustered_words = []\n",
        "for i in range(k):\n",
        "  words_per_cluster = []\n",
        "  for ind in order_centroids[i, :10]:\n",
        "    words_per_cluster.append(terms[ind])\n",
        "  clustered_words.append(words_per_cluster)\n",
        "\n",
        "print(clustered_words)"
      ],
      "id": "UyMTI42NCWxW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aiIapzKCeGZ"
      },
      "source": [
        "# Visualisierung\n",
        "import networkx as nx\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "\n",
        "\n",
        "colors = ['#dbc4fc','#5df243','#f594fc','#cea939','#585ff4','#e8dd7a','#bf4255',\n",
        "'#ccf1ff','#4393c1','#ccf6ff','#c202d3','#a7a9e5','#d8adf4','#d35e10','#96a30d',\n",
        "'#80fcf0','#eda6bc','#c66237','#ffb5b6','#fcfab5']\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "words = []\n",
        "color_map = []\n",
        "\n",
        "for i in range(len(clustered_words)):\n",
        "  cluster = clustered_words[i]\n",
        "  for word in cluster:\n",
        "    if not G.has_node(word):\n",
        "      G.add_node(word)\n",
        "      color_map.append(colors[i])\n",
        "\n",
        "    words.append(word)\n",
        "\n",
        "for word1 in words:\n",
        "  for word2 in words:\n",
        "    for corpus in pca_tokenized:\n",
        "      if (word1 in corpus) and (word2 in corpus):\n",
        "        if G.has_edge(word1, word2):\n",
        "          G[word1][word2]['weight'] += 0.5\n",
        "        elif G.has_edge(word2, word1):\n",
        "          G[word2][word1]['weight'] += 0.5\n",
        "        else:\n",
        "          G.add_edge(word1, word2, weight=0.5)\n",
        "\n",
        "pos = nx.spring_layout(G, scale=2)\n",
        "nx.draw(G, pos=pos, with_labels=True, node_color=color_map)"
      ],
      "id": "6aiIapzKCeGZ",
      "execution_count": null,
      "outputs": []
    }
  ]
}